{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividend Growth Investment Data\n",
    "\n",
    "Dividend growth investing has been around for ages and has always been a widely appreciated way of receiving passive income. As with any other investment, an elaborated decision needs to be made which stocks to pick. Also as with any other investment: the higher the risk, the higher the potential reward. This dataset should support the decision for a portfolio selection driven by data.\n",
    "\n",
    "This notebook will download all relevant data from robinhood.com and dividend.com for dividend investors.    \n",
    "The idea is to calculate a most realistic risk/reward score.    \n",
    "It can be merged with any other data set based on the symbol.   \n",
    "\n",
    "The approach is to download everything from robinhood.com, filter for only tradable stocks that pay dividend, then add their fundamental data, add ratings that robinhood.com offer (including the text for a rating itself), add popularity (basically how often this stock is in portfolios of robinhood users and add news that are displayed on robinhood.com. Lastly go through each stock symbol and look for a corresponding page on dividend.com. It turns out that only 7% of the robinhood dividend paying stocks can be found on dividend.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import time #to measure times\n",
    "import json #to handle the json respons\n",
    "import pandas as pd #for data wrangling\n",
    "import numpy as np #for certain dtypes\n",
    "from urllib.request import Request, urlopen #to perform URL get requests\n",
    "from urllib.parse import urlencode #to URL encode a payload\n",
    "from datetime import datetime, timedelta #for datetime conversions\n",
    "from lxml import etree #to read the xml tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions\n",
    "\n",
    "# This chunker function will help us loop through long lists of stock symbols without downloading every single one of them\n",
    "def chunker(seq, size):\n",
    "    return (seq[pos:pos + size] for pos in range(0, len(seq), size))\n",
    "\n",
    "# This recursive function allows us to download all pages from a paginated robinhood API response until there is no next page left\n",
    "def robinhood_download_all(url, i = 1, top=None):\n",
    "    url = url.replace('com/ratings','com/midlands/ratings') #needed because the 'next' URL in an API response is actually wrong sometimes\n",
    "    data = json.load(urlopen(Request(url)))\n",
    "    df = pd.DataFrame.from_records(data['results'])\n",
    "    print('\\r{}. page downloaded: {}'.format(i,url),end='')\n",
    "    n = data['next']\n",
    "    if top:\n",
    "        if i >= top: \n",
    "            return df\n",
    "    if n:\n",
    "        return pd.concat([df, robinhood_download_all(n, i+1, top)])\n",
    "    else:\n",
    "        return df\n",
    "\n",
    "# This function loops through groups of a long list. We need this function for all API calls that allow a comma separated\n",
    "# list as payload, but to attach the whole list would be too long. So we chunk it in batches.\n",
    "def robinhood_download_list(base_url, l, length = 100, i = 1):\n",
    "    result_dfs = []\n",
    "    for group in chunker(l, length):\n",
    "        print('\\r{}. group downloaded: {}'.format(i,base_url),end='')\n",
    "        i+=1\n",
    "        result_dfs.append(\n",
    "            pd.DataFrame.from_records(\n",
    "                list(\n",
    "                    filter(\n",
    "                        None, \n",
    "                        json.load(\n",
    "                            urlopen(\n",
    "                                Request(base_url+','.join(group))\n",
    "                            )\n",
    "                        )['results']\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    return pd.concat(result_dfs)\n",
    "\n",
    "# This functions converts columns that contain a $ sign\n",
    "def convert (x):\n",
    "    try:\n",
    "        return float(x.strip('$'))\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111. page downloaded: https://api.robinhood.com/instruments/?cursor=cD00NQ%3D%3D\n",
      "Time elapsed for downloading instruments: 0:02:08.690918\n"
     ]
    }
   ],
   "source": [
    "# Download list of instruments\n",
    "start_time = time.time()\n",
    "instruments = robinhood_download_all('https://api.robinhood.com/instruments/')\n",
    "print('\\nTime elapsed for {}: {}'.format('downloading instruments',str(timedelta(seconds = time.time() - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only tradables\n",
    "instruments = instruments[instruments['tradeable']==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll need this number later\n",
    "len_all_tradable_stocks = len(instruments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of instruments: (7187, 20)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of instruments:',instruments.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72. group downloaded: https://api.robinhood.com/fundamentals/?symbols=\n",
      "Time elapsed for downloading fundamentals: 0:01:02.008592\n"
     ]
    }
   ],
   "source": [
    "# Download fundamentals for instruments\n",
    "start_time = time.time()\n",
    "fundamentals = robinhood_download_list('https://api.robinhood.com/fundamentals/?symbols=', list(instruments['symbol']), length = 100)\n",
    "print('\\nTime elapsed for {}: {}'.format('downloading fundamentals',str(timedelta(seconds = time.time() - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of fundamendals: (7163, 20)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of fundamendals:',fundamentals.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merge instruments and fundamentals\n",
    "instruments = pd.merge(instruments, fundamentals, how='left', left_on='url', right_on='instrument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select only the instruments that pay a dividend (we'll handle the rest of the numeric columns later, but here it's needed already)\n",
    "instruments['dividend_yield'] = pd.to_numeric(instruments['dividend_yield'])\n",
    "instruments = instruments[instruments['dividend_yield'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of instruments paying dividend and their fundamentals: (3651, 40). That's 50.80% of all tradable stocks\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of instruments paying dividend and their fundamentals: {}. That's {:.2f}% of all tradable stocks\".format(instruments.shape, len(instruments) * 100 / len_all_tradable_stocks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294. page downloaded: https://api.robinhood.com/midlands/ratings/?page=294\n",
      "Time elapsed for downloading and flatten ratings: 0:03:44.648327\n"
     ]
    }
   ],
   "source": [
    "# Download and flatten ratings\n",
    "start_time = time.time()\n",
    "ratings = robinhood_download_all('https://api.robinhood.com/midlands/ratings/')\n",
    "ratings = pd.concat([ratings.summary.apply(pd.Series), ratings.drop('summary', axis=1)], axis=1)\n",
    "print('\\nTime elapsed for {}: {}'.format('downloading and flatten ratings',str(timedelta(seconds = time.time() - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge instruments and ratings\n",
    "instruments = pd.merge(instruments, ratings, how='left', left_on='id', right_on='instrument_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74. popularity group downloaded: https://api.robinhood.com/instruments/popularity/?ids=08c418ea-41a1-4ca4-8127-2fb924999b09\n",
      "Time elapsed for downloading popularities: 0:01:54.307169\n"
     ]
    }
   ],
   "source": [
    "# Download popularities\n",
    "start_time = time.time()\n",
    "l = list(instruments['id'])\n",
    "\n",
    "popularities_dfs = []\n",
    "i = 1\n",
    "for group in chunker(l, 50):\n",
    "    url = 'https://api.robinhood.com/instruments/popularity/?' + urlencode([('ids',','.join(group))])\n",
    "    popularities_dfs.append(\n",
    "        pd.DataFrame.from_records(\n",
    "            list(\n",
    "                filter(\n",
    "                    None, \n",
    "                    json.load(\n",
    "                        urlopen(\n",
    "                            Request(url)\n",
    "                        )\n",
    "                    )['results']\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    print('\\r{}. popularity group downloaded: {}'.format(i,url),end='')\n",
    "    i+=1\n",
    "    time.sleep(0.01)\n",
    "    # This wait was necessary in my case to not run into rate limits\n",
    "    if (len(popularities_dfs)%50==0):\n",
    "        print('\\rwaiting...',end='')\n",
    "        time.sleep(60)\n",
    "popularities = pd.concat(popularities_dfs).reset_index(drop=True)\n",
    "print('\\nTime elapsed for {}: {}'.format('downloading popularities',str(timedelta(seconds = time.time() - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge instruments and popularities\n",
    "instruments = pd.merge(instruments, popularities, how='left', left_on='instrument', right_on='instrument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3651/3651 news downloaded (100.00%): https://api.robinhood.com/midlands/news/UGE/\n",
      "Time elapsed for downloading news: 0:36:57.108773\n"
     ]
    }
   ],
   "source": [
    "# Download news\n",
    "start_time = time.time()\n",
    "news = []\n",
    "for i,row in instruments.iterrows():\n",
    "    url = 'https://api.robinhood.com/midlands/news/'+row['symbol']+'/'\n",
    "    print('\\r{:}/{} news downloaded ({:.2f}%): {}'.format(i+1,len(instruments),(i+1)*100/len(instruments),url),end='')\n",
    "    news.append(json.load(urlopen(Request(url)))['results'])\n",
    "print('\\nTime elapsed for {}: {}'.format('downloading news',str(timedelta(seconds = time.time() - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments['news'] = news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    UGE downloaded, 3651/3651 (100.00%) instruments downloaded; 3475 (95.18%) not found\n",
      "Time elapsed for downloading data from dividend.com: 1:02:18.032782\n"
     ]
    }
   ],
   "source": [
    "# load data from dividend.com\n",
    "start_time = time.time()\n",
    "n = datetime.now()\n",
    "dividend_com_rows = [] # single rows will be added to one data frame later\n",
    "payout_histories = {} # a sub dict for the entire history of payouts for each instrument\n",
    "url = 'http://www.dividend.com/search/?q='\n",
    "# The following line is needed because the source code of the dividend.com website doesn't close\n",
    "# this comment properly. In order for the etree to properly read the html, we need to replace it\n",
    "faulty_comment = '<!--[if lte IE 9]>\\n<a class=\"mm-header-logo\" href=\"http://www.dividend.com/\"><img alt=\"Dividend logo\" src=\"/assets/dividend-logo-358fc66b2fe52772c6dad898b8c40050.png\" /></a>\\n<!--<![endif]--><!-->\\n'\n",
    "# Counter for the not found ones\n",
    "not_found=0\n",
    "for i,row in instruments.iterrows():\n",
    "    security = row['symbol']\n",
    "    try:\n",
    "        html = urlopen(Request(url+security)).read()\n",
    "        html = html.decode().replace(faulty_comment,'')\n",
    "        tree = etree.HTML(html)\n",
    "        values = {}\n",
    "        values['symbol'] = security\n",
    "        \n",
    "        # Get each piece of information from the correspoinding div on the website\n",
    "        metrics = {\n",
    "            'div_yield' : '//*[@id=\"stock-dashboard-collapse\"]/div[2]/div[1]/div/div[1]',\n",
    "            'annual_payout' : '//*[@id=\"stock-dashboard-collapse\"]/div[2]/div[2]/div/div[1]',\n",
    "            'payout_ratio' : '//*[@id=\"stock-dashboard-collapse\"]/div[2]/div[3]/div/div[1]',\n",
    "            'div_growth' : '//*[@id=\"stock-dashboard-collapse\"]/div[2]/div[4]/div/div[1]',\n",
    "            'price' : '//*[@id=\"snapshot-collapse\"]/div[1]/div[1]/div/div',\n",
    "            'annual_payout' : '//*[@id=\"stock-dashboard-collapse\"]/div[2]/div[2]/div/div[1]',\n",
    "            'percent_off_52_week_high' : '//*[@id=\"snapshot-collapse\"]/div[2]/div[4]/div/div/span',\n",
    "            'name' : '//*[@id=\"stock\"]/div[4]/div/div/h1/span',\n",
    "            'exchange' : '//*[@id=\"profile-collapse\"]/div[1]/div[1]/span',\n",
    "            'sector' : '//*[@id=\"profile-collapse\"]/div[1]/div[2]/a',\n",
    "            'industry' : '//*[@id=\"profile-collapse\"]/div[1]/div[3]/a'\n",
    "        }\n",
    "        for metric, xpath in metrics.items():\n",
    "            try:\n",
    "                e = tree.xpath(xpath)[0]\n",
    "            except:\n",
    "                value = None\n",
    "            try:\n",
    "                value = float(e.text.format().strip('%$yrs'))\n",
    "            except ValueError as ve:\n",
    "                value = e.text \n",
    "            except:\n",
    "                value = None\n",
    "            metrics[metric] = {\n",
    "                'xpath' : xpath,\n",
    "                'value' : value\n",
    "            }\n",
    "            values[metric] = value\n",
    "        value = None\n",
    "        e = None\n",
    "            \n",
    "        #Load the payout history into a DataFrame\n",
    "        try:\n",
    "            table = tree.xpath('//*[@id=\"payout-history-table-collapse\"]/div/table')[0]\n",
    "            payout_history = pd.read_html(etree.tostring(table).decode())[0]\n",
    "            date_fields = ['Declared Date','Ex-Dividend Date','Record Date','Pay Date ▼']\n",
    "\n",
    "            for date_field in date_fields:\n",
    "                payout_history[date_field] = pd.to_datetime(payout_history[date_field])\n",
    "            payout_history['Payout Amount'] = payout_history['Payout Amount'].apply(convert)\n",
    "            \n",
    "            # Calculate the closest paymen as date and in days difference\n",
    "            payout_history['recent_payment_days'] = payout_history['Pay Date ▼'].apply(lambda x: x.date() - n.date())\n",
    "            days_series = payout_history['recent_payment_days']\n",
    "            \n",
    "            # we need the following if statement in case there are multiple future pay dates already planned\n",
    "            if len(days_series[days_series>timedelta(days=0)]) > 0:\n",
    "                recent_pay_date = payout_history['Pay Date ▼'][max(days_series[days_series>timedelta(days=0)].index)]\n",
    "                recent_payment = payout_history['recent_payment_days'][max(days_series[days_series>timedelta(days=0)].index)]\n",
    "            else:\n",
    "                recent_pay_date = max(payout_history['Pay Date ▼'])\n",
    "                recent_payment = max(payout_history['recent_payment_days'])\n",
    "            \n",
    "            payout_histories[security] = payout_history\n",
    "\n",
    "            values['recent_pay_date'] = recent_pay_date\n",
    "            values['recent_payment'] = recent_payment\n",
    "            values['payout_history'] = payout_history\n",
    "        except Exception as e:\n",
    "            not_found+=1\n",
    "            \n",
    "            \n",
    "        dividend_com_rows.append(values)\n",
    "    except Exception as e:\n",
    "        #print('\\r'+security,'not found')\n",
    "        not_found+=1\n",
    "    print('\\r{:>7} downloaded, {}/{} ({:.2f}%) instruments downloaded; {} ({:.2f}%) not found'.format(security,i+1,len(instruments),(i+1)*100/len(instruments),not_found, not_found*100/(i+1)),end='')\n",
    "dividend_com = pd.DataFrame(dividend_com_rows)\n",
    "print('\\nTime elapsed for {}: {}'.format('downloading data from dividend.com',str(timedelta(seconds = time.time() - start_time))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have this information already in the instruments\n",
    "dividend_com.drop(['name','sector'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge instruments with dividend.com data\n",
    "instruments = pd.merge(instruments, dividend_com, how='left', left_on='symbol', right_on='symbol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a few useful derives\n",
    "instruments['percent_buy_ratings'] = instruments.apply(lambda x:x['num_buy_ratings']/sum(x[['num_buy_ratings','num_hold_ratings','num_sell_ratings']]), axis = 1)\n",
    "instruments['percent_sell_ratings'] = instruments.apply(lambda x:x['num_sell_ratings']/sum(x[['num_buy_ratings','num_hold_ratings','num_sell_ratings']]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining order and type of columns\n",
    "object_cols = [\n",
    "    'symbol',\n",
    "    'name',\n",
    "    'simple_name',\n",
    "    'ceo',\n",
    "    'description',\n",
    "    'id',\n",
    "    'instrument',\n",
    "    'instrument_id',\n",
    "    'bloomberg_unique',\n",
    "    'fundamentals',\n",
    "    'splits',\n",
    "    'quote',\n",
    "    'url'\n",
    "]\n",
    "\n",
    "category_cols = [\n",
    "    'country',\n",
    "    'state',\n",
    "    'headquarters_state',\n",
    "    'headquarters_city',\n",
    "    'market',\n",
    "    'exchange',\n",
    "    'sector',\n",
    "    'industry',\n",
    "    'type',\n",
    "    'tradability',\n",
    "    'tradeable'\n",
    "]\n",
    "\n",
    "float_cols = [\n",
    "    'price',\n",
    "    'open',\n",
    "    'high',\n",
    "    'low',\n",
    "    'high_52_weeks',\n",
    "    'low_52_weeks',\n",
    "    'percent_off_52_week_high',\n",
    "    'volume',\n",
    "    'average_volume',\n",
    "    'average_volume_2_weeks',\n",
    "    'dividend_yield',\n",
    "    'div_yield',\n",
    "    'div_growth',\n",
    "    'annual_payout',\n",
    "    'payout_ratio',\n",
    "    'num_employees',\n",
    "    'year_founded',\n",
    "    'shares_outstanding',\n",
    "    'market_cap',\n",
    "    'num_buy_ratings',\n",
    "    'num_hold_ratings',\n",
    "    'num_sell_ratings',\n",
    "    'percent_buy_ratings',\n",
    "    'percent_sell_ratings',\n",
    "    'num_open_positions',\n",
    "    'day_trade_ratio',\n",
    "    'maintenance_ratio',\n",
    "    'margin_initial_ratio',\n",
    "    'pe_ratio',\n",
    "    'min_tick_size'\n",
    "]\n",
    "\n",
    "date_time_cols = [\n",
    "    'list_date',\n",
    "    'recent_pay_date'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Applying the defined types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments[float_cols] = instruments[float_cols].apply(lambda x: pd.to_numeric(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in category_cols:\n",
    "    instruments[col] = instruments[col].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruments[date_time_cols] = instruments[date_time_cols].apply(lambda x: pd.to_datetime(x, errors='coerce'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ordering columns\n",
    "instruments = instruments[object_cols + category_cols + float_cols + date_time_cols + list(set(list(instruments.columns ))- set(object_cols + category_cols + float_cols + date_time_cols))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "symbol                               object\n",
      "name                                 object\n",
      "simple_name                          object\n",
      "ceo                                  object\n",
      "description                          object\n",
      "id                                   object\n",
      "instrument                           object\n",
      "instrument_id                        object\n",
      "bloomberg_unique                     object\n",
      "fundamentals                         object\n",
      "splits                               object\n",
      "quote                                object\n",
      "url                                  object\n",
      "country                            category\n",
      "state                              category\n",
      "headquarters_state                 category\n",
      "headquarters_city                  category\n",
      "market                             category\n",
      "exchange                           category\n",
      "sector                             category\n",
      "industry                           category\n",
      "type                               category\n",
      "tradability                        category\n",
      "tradeable                          category\n",
      "price                               float64\n",
      "open                                float64\n",
      "high                                float64\n",
      "low                                 float64\n",
      "high_52_weeks                       float64\n",
      "low_52_weeks                        float64\n",
      "percent_off_52_week_high            float64\n",
      "volume                              float64\n",
      "average_volume                      float64\n",
      "average_volume_2_weeks              float64\n",
      "dividend_yield                      float64\n",
      "div_yield                           float64\n",
      "div_growth                          float64\n",
      "annual_payout                       float64\n",
      "payout_ratio                        float64\n",
      "num_employees                       float64\n",
      "year_founded                        float64\n",
      "shares_outstanding                  float64\n",
      "market_cap                          float64\n",
      "num_buy_ratings                     float64\n",
      "num_hold_ratings                    float64\n",
      "num_sell_ratings                    float64\n",
      "percent_buy_ratings                 float64\n",
      "percent_sell_ratings                float64\n",
      "num_open_positions                    int64\n",
      "day_trade_ratio                     float64\n",
      "maintenance_ratio                   float64\n",
      "margin_initial_ratio                float64\n",
      "pe_ratio                            float64\n",
      "min_tick_size                       float64\n",
      "list_date                    datetime64[ns]\n",
      "recent_pay_date              datetime64[ns]\n",
      "news                                 object\n",
      "recent_payment              timedelta64[ns]\n",
      "ratings                              object\n",
      "payout_history                       object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(instruments.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as pickle\n",
    "instruments.to_pickle('instruments.p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as json\n",
    "instruments.to_json('instruments.json')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "instruments = pd.read_pickle('instruments.p')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
